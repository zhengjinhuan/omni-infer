From 2ba79e9302d1813d5fedc1a2194bd90059eed791 Mon Sep 17 00:00:00 2001
From: lantian7 <liuchun22@huawei.com>
Date: Tue, 12 Aug 2025 10:01:37 +0800
Subject: [PATCH] 1

---
 vllm/engine/arg_utils.py    | 8 ++++++++
 vllm/v1/engine/async_llm.py | 9 +++++----
 2 files changed, 13 insertions(+), 4 deletions(-)

diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 442e4100f..b25aa9ab2 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -287,6 +287,8 @@ class EngineArgs:
     pipeline_parallel_size: int = ParallelConfig.pipeline_parallel_size
     tensor_parallel_size: int = ParallelConfig.tensor_parallel_size
     data_parallel_size: int = ParallelConfig.data_parallel_size
+    data_parallel_rank: int = ParallelConfig.data_parallel_rank
+    data_parallel_rank_local: int = ParallelConfig.data_parallel_rank_local
     data_parallel_size_local: Optional[int] = None
     data_parallel_address: Optional[str] = None
     data_parallel_rpc_port: Optional[int] = None
@@ -603,6 +605,10 @@ class EngineArgs:
                                     **parallel_kwargs["tensor_parallel_size"])
         parallel_group.add_argument("--data-parallel-size", "-dp",
                                     **parallel_kwargs["data_parallel_size"])
+        parallel_group.add_argument("--data-parallel-rank", "-dp-rank",
+                                    **parallel_kwargs["data_parallel_rank"])
+        parallel_group.add_argument("--data-parallel-rank-local", "-dp-rank-local",
+                                    **parallel_kwargs["data_parallel_rank_local"])
         parallel_group.add_argument('--data-parallel-size-local',
                                     '-dpl',
                                     type=int,
@@ -1064,6 +1070,8 @@ class EngineArgs:
             pipeline_parallel_size=self.pipeline_parallel_size,
             tensor_parallel_size=self.tensor_parallel_size,
             data_parallel_size=self.data_parallel_size,
+            data_parallel_rank=self.data_parallel_rank,
+            data_parallel_rank_local=self.data_parallel_rank_local,
             data_parallel_size_local=data_parallel_size_local,
             data_parallel_master_ip=data_parallel_address,
             data_parallel_rpc_port=data_parallel_rpc_port,
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index 74c2251c7..16165b52e 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -115,10 +115,11 @@ class AsyncLLM(EngineClient):
         self.output_processor = OutputProcessor(self.tokenizer,
                                                 log_stats=self.log_stats)
 
-        # EngineCore (starts the engine in background process).
-        core_client_class = AsyncMPClient if (
-            vllm_config.parallel_config.data_parallel_size
-            == 1) else DPAsyncMPClient
+        # # EngineCore (starts the engine in background process).
+        # core_client_class = AsyncMPClient if (
+        #     vllm_config.parallel_config.data_parallel_size
+        #     == 1) else DPAsyncMPClient
+        core_client_class = AsyncMPClient # use AsyncMPClient only. a workaround for manual api-server scaleout
 
         self.engine_core = core_client_class(
             vllm_config=vllm_config,
-- 
2.50.1.windows.1

