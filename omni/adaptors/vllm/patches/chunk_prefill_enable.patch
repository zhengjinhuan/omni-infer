diff --git a/vllm/v1/core/kv_cache_manager.py b/vllm/v1/core/kv_cache_manager.py
index 787a8825c..74313be9c 100644
--- a/vllm/v1/core/kv_cache_manager.py
+++ b/vllm/v1/core/kv_cache_manager.py
@@ -272,9 +272,10 @@ class KVCacheManager:
         # Speculated tokens might be rejected in the future, so we does
         # not cache any speculated tokens. We only cache blocks with
         # generated (accepted) tokens.
+        num_tokens_to_cache = min(num_computed_tokens + num_new_tokens, request.num_tokens)
         self.single_type_manager.cache_blocks(
             request, self.req_to_block_hashes[request.request_id],
-            num_computed_tokens + num_new_tokens - num_draft_tokens)
+            num_tokens_to_cache)
 
         return KVCacheBlocks(new_blocks)
 
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index e5456a3c0..5b7e09d4c 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -30,6 +30,10 @@ from vllm.v1.outputs import ModelRunnerOutput
 from vllm.v1.request import Request, RequestStatus
 from vllm.v1.spec_decode.metrics import SpecDecodingStats
 from vllm.v1.structured_output import StructuredOutputManager
+import os
+
+FORCE_ENABLE_CHUNK_PREFILL = os.getenv("FORCE_ENABLE_CHUNK_PREFILL", "0") == "1"
+
 
 logger = init_logger(__name__)
 
@@ -394,7 +398,10 @@ class Scheduler(SchedulerInterface):
                             < num_new_tokens):
                         num_new_tokens = (
                             self.scheduler_config.long_prefill_token_threshold)
-                    num_new_tokens = min(num_new_tokens, token_budget)
+                    if FORCE_ENABLE_CHUNK_PREFILL:
+                        num_new_tokens = min(num_new_tokens, token_budget)
+                    elif num_new_tokens > token_budget:
+                        break
                     assert num_new_tokens > 0
 
                     # Schedule encoder inputs.
