diff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py
index f96233545..ed05a5cc4 100644
--- a/vllm/entrypoints/openai/serving_chat.py
+++ b/vllm/entrypoints/openai/serving_chat.py
@@ -45,6 +45,9 @@ from vllm.transformers_utils.tokenizers import (maybe_serialize_tool_calls,
 
 logger = init_logger(__name__)
 
+import os
+reuse_prefilled_tokens = os.getenv("OMNI_REUSE_PREFILLED_TOKENS", "0") == "1"
+skip_decode_tokenize = os.getenv("OMNI_SKIP_DECODE_TOKENIZE", "0") == "1"
 
 class OpenAIServingChat(OpenAIServing):
 
@@ -211,6 +214,12 @@ class OpenAIServingChat(OpenAIServing):
         try:
             for i, engine_prompt in enumerate(engine_prompts):
                 sampling_params: Union[SamplingParams, BeamSearchParams]
+                if reuse_prefilled_tokens:
+                    if request.kv_transfer_params and "prompt_token_ids" in request.kv_transfer_params:
+                        engine_prompt["prefilled_token_ids"] = request.kv_transfer_params["prefilled_token"]
+                        new_tokens = tokenizer.convert_ids_to_tokens(engine_prompt["prefilled_token_ids"][0])
+                        delta_text = tokenizer.convert_tokens_to_string([new_tokens])
+                        engine_prompt["prefilled_texts"] = delta_text
                 default_max_tokens = self.max_model_len - len(
                     engine_prompt["prompt_token_ids"])
                 if request.use_beam_search:
@@ -916,6 +925,20 @@ class OpenAIServingChat(OpenAIServing):
 
         assert final_res is not None
 
+        if reuse_prefilled_tokens:
+            if request.kv_transfer_params and "prompt_token_ids" in request.kv_transfer_params:
+                prompt_token_ids = request.kv_transfer_params["prefilled_token"]
+                new_tokens = tokenizer.convert_ids_to_tokens(prompt_token_ids[0])
+                prompt_text = tokenizer.convert_tokens_to_string([new_tokens])
+                final_res.outputs[0].text = prompt_text + final_res.outputs[0].text
+        ## In Prefill node, the response will carry prompt_token_ids with kv_transfer_params
+        if skip_decode_tokenize:
+            if final_res.kv_transfer_params:
+                final_res.kv_transfer_params["prompt_token_ids"] = final_res.prompt_token_ids
+        if reuse_prefilled_tokens:
+            if final_res.kv_transfer_params:
+                final_res.kv_transfer_params["prefilled_token"] = [final_res.outputs[0].token_ids[0]]
+
         choices: list[ChatCompletionResponseChoice] = []
 
         role = self.get_chat_request_role(request)
@@ -1073,6 +1096,9 @@ class OpenAIServingChat(OpenAIServing):
             num_prompt_tokens += len(final_res.encoder_prompt_token_ids)
         num_generated_tokens = sum(
             len(output.token_ids) for output in final_res.outputs)
+        if reuse_prefilled_tokens:
+            if request.kv_transfer_params and "prompt_token_ids" in request.kv_transfer_params:
+                num_generated_tokens += 1
         usage = UsageInfo(prompt_tokens=num_prompt_tokens,
                           completion_tokens=num_generated_tokens,
                           total_tokens=num_prompt_tokens +
diff --git a/vllm/entrypoints/openai/serving_engine.py b/vllm/entrypoints/openai/serving_engine.py
index 8b3857fad..69366413b 100644
--- a/vllm/entrypoints/openai/serving_engine.py
+++ b/vllm/entrypoints/openai/serving_engine.py
@@ -75,6 +75,8 @@ from vllm.transformers_utils.tokenizer import AnyTokenizer, MistralTokenizer, ge
 from vllm.utils import (is_list_of, make_async, merge_async_iterators,
                         random_uuid, P, T)
 
+skip_decode_tokenize = os.getenv("OMNI_SKIP_DECODE_TOKENIZE", "0") == "1"
+
 logger = init_logger(__name__)
 
 CompletionLikeRequest = Union[CompletionRequest, DetokenizeRequest,
@@ -1009,24 +1011,29 @@ class OpenAIServing:
             request = tool_parser(tokenizer).adjust_request(  # type: ignore
                 request=request)
 
-        if isinstance(request_prompt, str):
-            prompt_inputs = await self._tokenize_prompt_input_async(
-                request,
-                tokenizer,
-                request_prompt,
-                truncate_prompt_tokens=truncate_prompt_tokens,
-                add_special_tokens=add_special_tokens,
-            )
+        if request.kv_transfer_params and "prompt_token_ids" in request.kv_transfer_params:
+            engine_prompt = TokensPrompt(
+                prompt_token_ids=request.kv_transfer_params["prompt_token_ids"])
         else:
-            # For MistralTokenizer
-            assert is_list_of(request_prompt, int), (
-                "Prompt has to be either a string or a list of token ids")
-            prompt_inputs = TextTokensPrompt(
-                prompt=tokenizer.decode(request_prompt),
-                prompt_token_ids=request_prompt)
-
-        engine_prompt = TokensPrompt(
-            prompt_token_ids=prompt_inputs["prompt_token_ids"])
+            if isinstance(request_prompt, str):
+                prompt_inputs = await self._tokenize_prompt_input_async(
+                    request,
+                    tokenizer,
+                    request_prompt,
+                    truncate_prompt_tokens=truncate_prompt_tokens,
+                    add_special_tokens=add_special_tokens,
+                )
+            else:
+                # For MistralTokenizer
+                assert is_list_of(request_prompt, int), (
+                    "Prompt has to be either a string or a list of token ids")
+                prompt_inputs = TextTokensPrompt(
+                    prompt=tokenizer.decode(request_prompt),
+                    prompt_token_ids=request_prompt)
+
+            engine_prompt = TokensPrompt(
+                prompt_token_ids=prompt_inputs["prompt_token_ids"])
+
         if mm_data is not None:
             engine_prompt["multi_modal_data"] = mm_data
         if request.mm_processor_kwargs is not None:
diff --git a/vllm/inputs/data.py b/vllm/inputs/data.py
index c83ab73b6..d0aeff466 100644
--- a/vllm/inputs/data.py
+++ b/vllm/inputs/data.py
@@ -7,7 +7,8 @@ from typing_extensions import NotRequired, TypedDict, TypeVar
 
 if TYPE_CHECKING:
     from vllm.multimodal.inputs import MultiModalDataDict, MultiModalInputs
-
+import os
+reuse_prefilled_tokens = os.getenv("OMNI_REUSE_PREFILLED_TOKENS", "0") == "1"
 
 class TextPrompt(TypedDict):
     """Schema for a text prompt."""
@@ -62,7 +63,12 @@ class TokensPrompt(TypedDict):
     """
     Optional cache salt to be used for prefix caching.
     """
-
+    if reuse_prefilled_tokens:
+        """
+        This is used when the model supports reusing prefilled tokens.
+        """
+        prefilled_token_ids: Optional[list[int]] = []
+        prefilled_texts: Optional[str] = ""
 
 class EmbedsPrompt(TypedDict):
     """Schema for a prompt provided via token embeddings."""
diff --git a/vllm/v1/core/sched/scheduler.py b/vllm/v1/core/sched/scheduler.py
index 123d9f627..b83489f6d 100644
--- a/vllm/v1/core/sched/scheduler.py
+++ b/vllm/v1/core/sched/scheduler.py
@@ -33,6 +33,8 @@ from vllm.v1.structured_output import StructuredOutputManager
 
 logger = init_logger(__name__)
 
+import os
+reuse_prefilled_tokens = os.getenv("OMNI_REUSE_PREFILLED_TOKENS", "0") == "1"
 
 class Scheduler(SchedulerInterface):
 
@@ -1005,6 +1007,11 @@ class Scheduler(SchedulerInterface):
         num_computed_tokens = len(block_ids) * self.block_size
         if num_computed_tokens == request.num_tokens:
             num_computed_tokens -= 1
+        
+        if reuse_prefilled_tokens:
+            if request.kv_transfer_params and "prefilled_token" in request.kv_transfer_params:
+                request.prompt_token_ids.extend(request.kv_transfer_params["prefilled_token"])
+                request.append_output_token_ids(request.kv_transfer_params["prefilled_token"])
 
         # with spec
         if self.vllm_config.speculative_config is not None:
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index 5628f0a11..d0f5fa508 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -16,7 +16,7 @@ from vllm.inputs.preprocess import InputPreprocessor
 from vllm.logger import init_logger
 from vllm.lora.request import LoRARequest
 from vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalRegistry
-from vllm.outputs import RequestOutput
+from vllm.outputs import RequestOutput, CompletionOutput
 from vllm.pooling_params import PoolingParams
 from vllm.prompt_adapter.request import PromptAdapterRequest
 from vllm.sampling_params import SamplingParams
@@ -35,6 +35,8 @@ from vllm.v1.executor.abstract import Executor
 from vllm.v1.metrics.loggers import (StatLoggerBase, StatLoggerFactory,
                                      setup_default_loggers)
 from vllm.v1.metrics.stats import IterationStats, SchedulerStats
+import os
+reuse_prefilled_tokens = os.getenv("OMNI_REUSE_PREFILLED_TOKENS", "0") == "1"
 
 logger = init_logger(__name__)
 
@@ -287,6 +289,30 @@ class AsyncLLM(EngineClient):
         """
 
         try:
+            if reuse_prefilled_tokens:
+                if "prefilled_token_ids" in prompt and prompt["prefilled_token_ids"] != []:
+                    if sampling_params.n == 1:
+                        output = RequestOutput(request_id=request_id,
+                                prompt=None, finished=False, prompt_logprobs=None,
+                                prompt_token_ids=prompt["prompt_token_ids"],
+                                outputs=[CompletionOutput(index=0,
+                                    cumulative_logprob=None, logprobs=None,
+                                    text= prompt["prefilled_texts"],
+                                    token_ids=prompt["prefilled_token_ids"])])
+                    else:
+                        # Fan out child requests (for n>1).
+                        parent_request = ParentRequest(request_id, sampling_params)
+                        for idx in range(sampling_params.n):
+                            request_id_child, params = parent_request.get_child_info(idx)
+                            output = RequestOutput(request_id=request_id_child,
+                                    prompt=None, finished=False, prompt_logprobs=None,
+                                    prompt_token_ids=prompt["prompt_token_ids"],
+                                    outputs=[CompletionOutput(index=idx,
+                                        cumulative_logprob=None, logprobs=None,
+                                        text= prompt["prefilled_texts"],
+                                        token_ids=prompt["prefilled_token_ids"])])
+                    prompt["prefilled_token_ids"] = []
+                    yield output
             # We start the output_handler on the first call to generate() so
             # we can call __init__ before the event loop, which enables us
             # to handle startup failure gracefully in the OpenAI server.
