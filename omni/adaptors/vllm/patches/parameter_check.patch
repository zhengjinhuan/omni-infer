diff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py
index b0e3fac30..834b010d9 100644
--- a/vllm/entrypoints/openai/protocol.py
+++ b/vllm/entrypoints/openai/protocol.py
@@ -6,7 +6,7 @@ import json
 import time
 from http import HTTPStatus
 from typing import Annotated, Any, ClassVar, Literal, Optional, Union
-
+import os
 import regex as re
 import torch
 from fastapi import HTTPException, UploadFile
@@ -612,12 +612,33 @@ class ChatCompletionRequest(OpenAIBaseModel):
     @model_validator(mode="before")
     @classmethod
     def check_logprobs(cls, data):
-        if (_ := data.get("prompt_logprobs")) is not None:
-            raise ValueError("`prompt_logprobs` are not supported")
+        vllm_plugins = os.getenv('VLLM_PLUGINS','').strip()
+        if (prompt_logprobs := data.get("prompt_logprobs")) is not None:
+            if 'npu' in vllm_plugins.split(','):
+                raise ValueError("`prompt_logprobs` are not supported")
+            else:
+                if data.get("stream") and prompt_logprobs > 0:
+                    raise ValueError(
+                        "`prompt_logprobs` are not available when `stream=True`.")
+
+                if prompt_logprobs < 0:
+                    raise ValueError("`prompt_logprobs` must be a positive value.")
+
+        if (top_logprobs := data.get("top_logprobs")) is not None:
+            if 'npu' in vllm_plugins.split(','):
+                raise ValueError("`top_logprobs` are not supported")
+            else:
+                if top_logprobs < 0:
+                    raise ValueError("`top_logprobs` must be a positive value.")
+                if top_logprobs > 0 and not data.get("logprobs"):
+                    raise ValueError(
+                        "when using `top_logprobs`, `logprobs` must be set to true."
+                    )
+
         if(_ := data.get("logprobs")) is not None:
-            raise ValueError("`logprobs` are not supported")
-        if(_ := data.get("top_logprobs")) is not None:
-            raise ValueError("`top_logprobs` are not supported")
+            if 'npu' in vllm_plugins.split(','):
+                raise ValueError("`logprobs` are not supported")
+
         return data
 
     @model_validator(mode="before")
@@ -993,10 +1014,25 @@ class CompletionRequest(OpenAIBaseModel):
     @model_validator(mode="before")
     @classmethod
     def check_logprobs(cls, data):
-        if (_ := data.get("prompt_logprobs")) is not None:
-            raise ValueError("`prompt_logprobs` is not supported")
-        if (_ := data.get("logprobs")) is not None:
-            raise ValueError("`logprobs` is not supported")
+        vllm_plugins = os.getenv('VLLM_PLUGINS','').strip()
+        if (prompt_logprobs := data.get("prompt_logprobs")) is not None:
+            if 'npu' in vllm_plugins.split(','):
+                raise ValueError("`prompt_logprobs` is not supported")
+            else:
+                if data.get("stream") and prompt_logprobs > 0:
+                    raise ValueError(
+                        "`prompt_logprobs` are not available when `stream=True`.")
+
+                if prompt_logprobs < 0:
+                    raise ValueError("`prompt_logprobs` must be a positive value.")
+
+        if (logprobs := data.get("logprobs")) is not None:
+            if 'npu' in vllm_plugins.split(','):
+                raise ValueError("`logprobs` is not supported")
+            else:
+                if logprobs < 0:
+                    raise ValueError("`logprobs` must be a positive value.")
+
         return data
 
     @model_validator(mode="before")
diff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py
index 943abbed4..949325d82 100644
--- a/vllm/entrypoints/openai/serving_chat.py
+++ b/vllm/entrypoints/openai/serving_chat.py
@@ -225,9 +225,14 @@ class OpenAIServingChat(OpenAIServing):
                         engine_prompt["prefilled_texts"] = delta_text
                 default_max_tokens = self.max_model_len - len(
                     engine_prompt["prompt_token_ids"])
+                
+                vllm_plugins = os.getenv('VLLM_PLUGINS','').strip()
                 if request.use_beam_search:
-                    sampling_params = request.to_beam_search_params(
-                        default_max_tokens, self.default_sampling_params)
+                    if 'npu' in vllm_plugins.split(','):
+                        raise ValueError("Beam search is not supported in this deployment. ")
+                    else:
+                        sampling_params = request.to_beam_search_params(
+                            default_max_tokens, self.default_sampling_params)
                 else:
                     sampling_params = request.to_sampling_params(
                         default_max_tokens,
diff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py
index a162a01ee..eaaf8beef 100644
--- a/vllm/entrypoints/openai/serving_completion.py
+++ b/vllm/entrypoints/openai/serving_completion.py
@@ -5,7 +5,7 @@ import time
 from collections.abc import AsyncGenerator, AsyncIterator
 from collections.abc import Sequence as GenericSequence
 from typing import Optional, Union, cast
-
+import os
 import jinja2
 from fastapi import Request
 from typing_extensions import assert_never
@@ -162,16 +162,18 @@ class OpenAIServingCompletion(OpenAIServing):
                 else:
                     assert_never(engine_prompt)
                 default_max_tokens = self.max_model_len - input_length
-
+                vllm_plugins = os.getenv('VLLM_PLUGINS','').strip()
                 if request.use_beam_search:
-                    sampling_params = request.to_beam_search_params(
-                        default_max_tokens, self.default_sampling_params)
+                    if 'npu' in vllm_plugins.split(','):
+                        raise ValueError("Beam search is not supported in this deployment. ")
+                    else:
+                        sampling_params = request.to_beam_search_params(
+                            default_max_tokens, self.default_sampling_params)
                 else:
                     sampling_params = request.to_sampling_params(
                         default_max_tokens,
                         self.model_config.logits_processor_pattern,
                         self.default_sampling_params)
-
                 request_id_item = f"{request_id}-{i}"
 
                 self._log_inputs(request_id_item,
diff --git a/vllm/sampling_params.py b/vllm/sampling_params.py
index 65c019d78..b2f414889 100644
--- a/vllm/sampling_params.py
+++ b/vllm/sampling_params.py
@@ -5,7 +5,7 @@ from dataclasses import dataclass
 from enum import Enum, IntEnum
 from functools import cached_property
 from typing import Annotated, Any, Optional, Union
-
+import os
 import msgspec
 from pydantic import BaseModel
 from typing_extensions import deprecated
@@ -384,11 +384,18 @@ class SamplingParams(
         self._all_stop_token_ids.update(self.stop_token_ids)
 
     def _verify_args(self) -> None:
+        vllm_plugins = os.getenv('VLLM_PLUGINS','').strip()
         if not isinstance(self.n, int):
             raise ValueError(f"n must be an int, but is of "
                              f"type {type(self.n)}")
-        if self.n < 1:
-            raise ValueError(f"n must be at least 1, got {self.n}.")
+        
+        if 'npu' in vllm_plugins.split(','):
+            if self.n != 1:
+                raise ValueError(f"n must be at least 1 in npu_version, got {self.n}.")
+        else:
+            if self.n < 1:
+                raise ValueError(f"n must be at least 1, got {self.n}.")
+
         if not -2.0 <= self.presence_penalty <= 2.0:
             raise ValueError("presence_penalty must be in [-2, 2], got "
                              f"{self.presence_penalty}.")
@@ -424,12 +431,29 @@ class SamplingParams(
             raise ValueError(
                 f"min_tokens must be less than or equal to "
                 f"max_tokens={self.max_tokens}, got {self.min_tokens}.")
+        
         if self.logprobs is not None:
-            raise ValueError("logprobs is not supported.")
+            if 'npu' in vllm_plugins.split(','):
+                raise ValueError("logprobs is not supported.")
+            else:
+                if self.logprobs < 0:
+                    raise ValueError(
+                        f"logprobs must be non-negative, got {self.logprobs}.")
         if self.prompt_logprobs is not None:
-            raise ValueError("prompt_logprobs is not supported.")
+            if 'npu' in vllm_plugins.split(','):
+                raise ValueError("prompt_logprobs is not supported.")
+            else:
+                if self.prompt_logprobs < 0:
+                    raise ValueError(f"prompt_logprobs must be non-negative, got "
+                                    f"{self.prompt_logprobs}.")
         if self.truncate_prompt_tokens is not None:
-            raise ValueError("truncate_prompt_tokens is not supported.")
+            if 'npu' in vllm_plugins.split(','):
+                raise ValueError("truncate_prompt_tokens is not supported.")
+            else:
+                if self.truncate_prompt_tokens < 1:
+                    raise ValueError(f"truncate_prompt_tokens must be >= 1, "
+                                    f"got {self.truncate_prompt_tokens}")
+
         assert isinstance(self.stop_token_ids, list)
         if not all(isinstance(st_id, int) for st_id in self.stop_token_ids):
             raise ValueError(f"stop_token_ids must contain only integers, "
@@ -444,8 +468,9 @@ class SamplingParams(
         if self.best_of != self._real_n and self.output_kind == (
                 RequestOutputKind.DELTA):
             raise ValueError("best_of must equal n to use output_kind=DELTA")
-        if self.guided_decoding is not None:
-            raise ValueError("guided decoding is not supported.")
+        if 'npu' in vllm_plugins.split(','):
+            if self.guided_decoding is not None:
+                raise ValueError("guided decoding is not supported.")
 
     def _verify_greedy_sampling(self) -> None:
         if self.n > 1:
