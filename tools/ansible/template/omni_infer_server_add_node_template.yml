# Copyright 2025 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

- name: run omniai
  hosts: all
  any_errors_fatal: true
  max_fail_percentage: 0
  gather_facts: yes

  environment:
    # Global Configuration
    LOG_PATH: "/data/log_path"
    MODEL_PATH: "/data/models/DeepSeek-R1-w8a8-fusion"
    MODEL_LEN_MAX_PREFILL: "32000"
    MODEL_LEN_MAX_DECODE: "16384"
    LOG_PATH_IN_EXECUTOR: "/data/log_path_in_executor"
    CODE_PATH: "/data/local_code_path"
    KV_CONNECTOR: "LMCacheConnectorV1"

    # Configuration for containers
    DOCKER_IMAGE_ID: "REPOSITORY:TAG"
    DOCKER_NAME_P: "you_name_omni_infer_prefill"
    DOCKER_NAME_D: "you_name_omni_infer_decode"
    DOCKER_NAME_C: "you_name_omni_infer_proxy"
    SCRIPTS_PATH: "/tmp/scripts_path"

    #Configuration for lb_sdk in global proxy
    PREFILL_LB_SDK: "pd_score_balance"
    DECODE_LB_SDK: "pd_score_balance"

    # Tensor Parallel Size
    DECODE_TENSOR_PARALLEL_SIZE: "1"

  vars:

    docker_run_cmd: |
      docker run -it --shm-size=500g \
        -e LOG_PATH=$LOG_PATH \
        --net=host \
        --privileged=true \
        -u root \
        -w /data \
        --device=/dev/davinci_manager \
        --device=/dev/hisi_hdc \
        --device=/dev/devmm_svm \
        --entrypoint=bash \
        -v /data:/data \
        -v /tmp:/tmp \
        -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
        -v /usr/local/dcmi:/usr/local/dcmi \
        -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
        -v /etc/ascend_install.info:/etc/ascend_install.info \
        -v /usr/local/sbin:/usr/local/sbin \
        -v /etc/hccn.conf:/etc/hccn.conf \
        -v /usr/bin/hccn_tool:/usr/bin/hccn_tool \
        -v $LOG_PATH:$LOG_PATH \
        -v $MODEL_PATH:$MODEL_PATH \
        -v $SCRIPTS_PATH:$SCRIPTS_PATH \
        -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \

    docker_exec_cmd: |
      docker exec \

    run_vllm_server_prefill_cmd: |
      #!/bin/bash

      . ~/.bashrc
      export HCCL_CONNECT_TIMEOUT=1800
      export HCCL_EXEC_TIMEOUT=1800
      export ASCEND_GLOBAL_LOG_LEVEL=3
      prefill_server_list=$(echo "$PREFILL_SERVER_LIST" | awk '$1=$1' | tr -d ',')
      KV_PARALLEL_SIZE=$((PREFILL_POD_NUM + 1))
      MODEL_EXTRA_CFG_PATH="/workspace/omniinfer/tests/test_config/test_config_prefill.json"
      EXTRA_ARGS='--max-num-batched-tokens 30000 --enforce-eager --enable-expert-parallel --disable-log-requests --max-num-seqs 16 --no-enable-prefix-caching'
      GPU_UTIL=0.92
      VLLM_ENABLE_MC2=1
      HCCL_OP_EXPANSION_MODE="AIV"
      export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
      unset https_proxy
      unset http_proxy
      unset proxy
      
      # ems env adapter
      # export ENABLE_VLLM_EMS=0
      # export EMS_STORE_LOCAL=1
      # export MODEL_ID=cc_kvstore@_@ds_default_ns_001
      # export ACCELERATE_ID=cc_kvstore@_@ds_default_ns_001

      kv_role="kv_producer"
      if [ ${KV_CONNECTOR} == "LMCacheConnectorV1" ]; then
        export MOONCAKE_CONFIG_PATH=$MOONCAKE_CONFIG_PATH
        export LMCACHE_CONFIG_FILE=$LMCACHE_MOONCAKE_CONFIG_PATH
        kv_role="kv_both"
      fi

      if [ $(echo -n "$NODE_IP_LIST" | tr -cd ',' | wc -c) -ge 1 ]; then
        export MASTER_NODE_IP=$HOST_IP
        export NNODES=$NNODES
        export NODE_RANK=$NODE_RANK
        export NODE_IP_LIST=$NODE_IP_LIST
        export RAY_EXPERIMENTAL_NOSET_ASCEND_RT_VISIBLE_DEVICES=1
        export RAY_CGRAPH_get_timeout=7200
        ray stop --force
        EXTRA_ARGS="${EXTRA_ARGS} --distributed-executor-backend ray"
        node_count=$(( $(echo -n "$NODE_IP_LIST" | tr -cd ',' | wc -c) + 1 ))
        PREFILL_TENSOR_PARALLEL_SIZE=$(( PREFILL_TENSOR_PARALLEL_SIZE * node_count ))
      fi

      # install omni_placement
      pip uninstall omni_placement -y

      cd /workspace/omniinfer/tools/scripts
      bash /workspace/omniinfer/tools/scripts/pd_run.sh \
        --local-decode-server-ip-list "$SERVER_IP_LIST" \
        --global-decode-server-ip-list "$SERVER_IP_LIST" \
        --prefill-pod-num ${PREFILL_POD_NUM} \
        --gloo-socket-ifname ${SOCKET_IFNAME} \
        --tp-socket-ifname ${SOCKET_IFNAME} \
        --model-path ${MODEL_PATH} \
        --master-ip ${HOST_IP} \
        --role "prefill" \
        --kv-role ${kv_role} \
        --max-model-len ${MODEL_LEN_MAX_PREFILL} \
        --master-port ${MASTER_PORT} \
        --base-api-port ${API_PORT} \
        --tp ${PREFILL_TENSOR_PARALLEL_SIZE} \
        --ascend-rt-visible-devices "${PREFILL_SERVER_LIST}" \
        --kv-rank ${KV_RANK} \
        --kv-engine-id ${KV_RANK} \
        --kv-parallel-size ${KV_PARALLEL_SIZE} \
        --kv-connector ${KV_CONNECTOR} \
        --model-extra-cfg-path ${MODEL_EXTRA_CFG_PATH} \
        --gpu-util ${GPU_UTIL} \
        --vllm-enable-mc2 ${VLLM_ENABLE_MC2} \
        --extra-args "${EXTRA_ARGS}" \
        --hccl-buffsize "${HCCL_BUFFSIZE}" \
        --hccl-op-expansion-mode "${HCCL_OP_EXPANSION_MODE}" \
        --log-dir "${LOG_PATH}/{{ inventory_hostname }}" > ${LOG_PATH}/{{ inventory_hostname }}/run_prefill.log 2>&1 &
    
    run_vllm_server_decode_cmd: |
      #!/bin/bash

      . ~/.bashrc
      HCCL_BUFFSIZE=1000
      export HCCL_CONNECT_TIMEOUT=1800
      export HCCL_EXEC_TIMEOUT=1800
      export ASCEND_GLOBAL_LOG_LEVEL=3
      dp=$(echo -n "$DECODE_DATA_PARALLEL_SIZE" | tr -cd ',' | wc -c)
      ((dp++))
      KV_PARALLEL_SIZE=$((PREFILL_POD_NUM + 1))

      declare -A config_dict=(
          {% for key, value in server_offset_dict.items() %}
          [{{ key }}]={{ value }}{% if not loop.last %} {% endif %}
          {% endfor %}
      )
      MODEL_EXTRA_CFG_PATH="/workspace/omniinfer/tests/test_config/test_config_decode.json"
      EXTRA_ARGS='--enable-expert-parallel --disable-log-requests --max-num-seqs 32 --no-enable-prefix-caching'
      GPU_UTIL=0.92
      ADDITIONAL_CONFIG='{"graph_model_compile_config": {"level":1}}'
      VLLM_ENABLE_MC2=1
      HCCL_OP_EXPANSION_MODE="AIV"
      export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
      unset https_proxy
      unset http_proxy
      unset proxy

      if [ ${KV_CONNECTOR} == "LMCacheConnectorV1" ]; then
        export MOONCAKE_CONFIG_PATH=$MOONCAKE_CONFIG_PATH
        export LMCACHE_CONFIG_FILE=$LMCACHE_MOONCAKE_CONFIG_PATH
      fi
      
      # install omni_placement
      pip uninstall omni_placement -y
      
      python /workspace/omniinfer/tools/scripts/process_nz_config.py /usr/local/Ascend/ascend-toolkit/latest/opp/built-in/op_impl/ai_core/tbe/config/ascend910_93/aic-ascend910_93-ops-info.json

      cd /workspace/omniinfer/tools/scripts
      bash /workspace/omniinfer/tools/scripts/pd_run.sh \
        --local-decode-server-ip-list "$SERVER_IP_LIST" \
        --global-decode-server-ip-list "$SERVER_IP_LIST" \
        --prefill-pod-num ${PREFILL_POD_NUM} \
        --gloo-socket-ifname ${SOCKET_IFNAME} \
        --tp-socket-ifname ${SOCKET_IFNAME} \
        --num-servers ${NUM_SERVERS} \
        --num-dp ${dp} \
        --server-offset ${config_dict[$HOST]:-0} \
        --model-path ${MODEL_PATH} \
        --master-ip ${HOST_IP} \
        --role "decode" \
        --kv-role "kv_consumer" \
        --max-model-len ${MODEL_LEN_MAX_DECODE} \
        --master-port ${MASTER_PORT} \
        --base-api-port ${API_PORT} \
        --tp ${DECODE_TENSOR_PARALLEL_SIZE} \
        --kv-rank ${PREFILL_POD_NUM} \
        --kv-engine-id ${PREFILL_POD_NUM} \
        --kv-parallel-size ${KV_PARALLEL_SIZE} \
        --kv-connector ${KV_CONNECTOR} \
        --model-extra-cfg-path ${MODEL_EXTRA_CFG_PATH} \
        --gpu-util ${GPU_UTIL} \
        --additional-config "$ADDITIONAL_CONFIG" \
        --vllm-enable-mc2 ${VLLM_ENABLE_MC2} \
        --extra-args "${EXTRA_ARGS}" \
        --hccl-buffsize "${HCCL_BUFFSIZE}" \
        --hccl-op-expansion-mode "${HCCL_OP_EXPANSION_MODE}" \
        --log-dir "${LOG_PATH}/{{ inventory_hostname }}" > ${LOG_PATH}/{{ inventory_hostname }}/run_decode.log 2>&1 &

    run_proxy_cmd: |
      #!/bin/bash

      prefill_result="{{ PREFILL_API_SERVER_LIST }}"
      prefill_result=`echo "$prefill_result" | awk '$1=$1'`

      decode_result=""
      decode_api_servers="{{ DECODE_API_SERVER_LIST }}"
      decode_api_servers=`echo "$decode_api_servers" | awk '$1=$1'`
      decode_array=(${decode_api_servers//,/ })
      for var in ${decode_array[@]}; do
        address=${var%@*}
        ip=${address%:*}
        port=${address#*:}
        num=${var#*@}
        for ((i=0; i<=$num;i++)); do
          if [[ -z ${decode_result} ]]; then
            decode_result="$ip:$port"
          else
            decode_result="${decode_result},$ip:$port"
          fi
          ((port++))
        done
      done

      cd /workspace/omniinfer/tools/scripts
      bash global_proxy.sh \
        --listen-port "$PROXY_NODE_PORT" \
        --prefill-servers-list "$prefill_result" \
        --decode-servers-list "$decode_result" \
        --log-file ${LOG_PATH}/{{ inventory_hostname }}/nginx_error.log \
        --log-level notice \
        --core-num 4 \
        --start-core-index 16 \
        --prefill-lb-sdk ${PREFILL_LB_SDK} \
        --decode-lb-sdk ${DECODE_LB_SDK} \
        --prefill-max-num-seqs 16 \
        --decode-max-num-seqs 32

    generate_mooncake_config_json: |
      {
        "metadata_server": "etcd://{{ ETCD_IP }}:{{ ETCD_PORT }}",
        "local_hostname": "{{ HOST_IP }}",
        "protocol": "tcp",
        "transfer_timeout": 60,
        "global_segment_size": 17179869184,
        "master_server_address": "{{ MOONCAKE_MASTER_IP }}:{{ MOONCAKE_MASTER_PORT }}"
      }

    generate_lmcache_mooncake_config_yml: |
      # 256 Tokens per kV Chunk
      chunk_size: 256
      # Enable CPU memory backend
      local_cpu: false # default
      # 5GB of Pinned CPU memory
      max_local_cpu_size: 5.0 # default
      
      remote_url: "mooncakestore://{{ MOONCAKE_MASTER_IP }}:{{ MOONCAKE_MASTER_PORT }}"
      remote_serde: "naive"
      external_lookup_client: "mooncakestore://{{ MOONCAKE_MASTER_IP }}:{{ MOONCAKE_MASTER_PORT }}"

      extra_config:
        retmote_enable_mla_worker_id_as0: true

    kill_python_processes_cmd: |
      #!/bin/bash

      ps aux | grep "python" | grep -v "grep" | awk '{print $2}' | xargs kill -9

    kill_nginx_processes_cmd: |
      #!/bin/bash

      ps aux | grep "nginx" | grep -v "grep" | awk '{print $2}' | xargs kill -9

    kill_ray_processes_cmd: |
      #!/bin/bash

      ps aux | grep "ray" | grep -v "grep" | awk '{print $2}' | xargs kill -9

    start_docker_cmd_p: >
      {{ docker_run_cmd }}
      -d --name $DOCKER_NAME_P $DOCKER_IMAGE_ID

    start_docker_cmd_d: >
      {{ docker_run_cmd }}
      -d --name $DOCKER_NAME_D $DOCKER_IMAGE_ID

    start_docker_cmd_c: >
      {{ docker_run_cmd }}
      -e PROXY_NODE_PORT=$NODE_PORT
      -d --name $DOCKER_NAME_C $DOCKER_IMAGE_ID

    docker_start_vllm_cmd_p: >
      {{ docker_exec_cmd }}
      -e MODEL_PATH=$MODEL_PATH
      -e MODEL_LEN_MAX_PREFILL=$MODEL_LEN_MAX_PREFILL
      -e PREFILL_SERVER_LIST=$PREFILL_SERVER_LIST
      -e PREFILL_TENSOR_PARALLEL_SIZE=$PREFILL_TENSOR_PARALLEL_SIZE
      -e IP=$IP
      -e HOST_IP=$HOST_IP
      -e MASTER_PORT=$NODE_PORT
      -e API_PORT=$API_PORT
      -e SERVER_IP_LIST=$SERVER_IP_LIST
      -e PREFILL_POD_NUM=$PREFILL_POD_NUM
      -e SOCKET_IFNAME=$SOCKET_IFNAME
      -e KV_RANK=$KV_RANK
      -e NNODES=$NNODES
      -e NODE_RANK=$NODE_RANK
      -e NODE_IP_LIST=$NODE_IP_LIST
      -e KV_CONNECTOR=$KV_CONNECTOR
      -e MOONCAKE_CONFIG_PATH=$MOONCAKE_CONFIG_PATH
      -e LMCACHE_MOONCAKE_CONFIG_PATH=$LMCACHE_MOONCAKE_CONFIG_PATH
      -d $DOCKER_NAME_P
      /bin/bash -c $SCRIPTS_PATH/vllm_run_for_p.sh

    docker_start_vllm_cmd_d: >
      {{ docker_exec_cmd }}
      -e MODEL_PATH=$MODEL_PATH
      -e MODEL_LEN_MAX_DECODE=$MODEL_LEN_MAX_DECODE
      -e DECODE_SERVER_LIST=$DECODE_SERVER_LIST
      -e DECODE_TENSOR_PARALLEL_SIZE=$DECODE_TENSOR_PARALLEL_SIZE
      -e DECODE_DATA_PARALLEL_SIZE=$DECODE_DATA_PARALLEL_SIZE
      -e HOST_IP=$HOST_IP
      -e MASTER_PORT=$NODE_PORT
      -e API_PORT=$API_PORT
      -e SERVER_IP_LIST=$SERVER_IP_LIST
      -e PREFILL_POD_NUM=$PREFILL_POD_NUM
      -e DECODE_POD_NUM=$DECODE_POD_NUM
      -e SOCKET_IFNAME=$SOCKET_IFNAME
      -e NUM_SERVERS=$NUM_SERVERS
      -e HOST=$HOST
      -e KV_CONNECTOR=$KV_CONNECTOR
      -e MOONCAKE_CONFIG_PATH=$MOONCAKE_CONFIG_PATH
      -e LMCACHE_MOONCAKE_CONFIG_PATH=$LMCACHE_MOONCAKE_CONFIG_PATH
      -d $DOCKER_NAME_D
      /bin/bash -c $SCRIPTS_PATH/vllm_run_for_d.sh

    docker_start_proxy_cmd_c: >
      {{ docker_exec_cmd }}
      -e PREFILL_LB_SDK=$PREFILL_LB_SDK
      -e DECODE_LB_SDK=$DECODE_LB_SDK
      -d $DOCKER_NAME_C
      /bin/bash -c $SCRIPTS_PATH/run_proxy_server.sh

    docker_cp_prefill_code_cmd: "docker cp {{ ansible_env.CODE_PATH }}/omniinfer $DOCKER_NAME_P:/workspace/"
    docker_cp_decode_code_cmd: "docker cp {{ ansible_env.CODE_PATH }}/omniinfer $DOCKER_NAME_D:/workspace/"
    docker_cp_proxy_code_cmd: >
      [ "${KV_CONNECTOR}" != "LMCacheConnectorV1" ] || { docker cp {{ ansible_env.CODE_PATH }}/omniinfer $DOCKER_NAME_C:/workspace/;}
    docker_update_proxy_code_cmd: >
      {{ docker_exec_cmd }}
      -e KV_CONNECTOR=$KV_CONNECTOR
      $DOCKER_NAME_C
      /bin/bash -c '[ "${KV_CONNECTOR}" != "LMCacheConnectorV1" ] || { source ~/.bashrc && cd /workspace/omniinfer/omni/adaptors/lmcache/script && bash install.sh &> ${LOG_PATH}/install.log;}'
    docker_update_prefill_code_cmd: >
      {{ docker_exec_cmd }}
      -e KV_CONNECTOR=$KV_CONNECTOR
      $DOCKER_NAME_P
      /bin/bash -c '. ~/.bashrc && cd /workspace/omniinfer/infer_engines && git config --global --add safe.directory /workspace/omniinfer/infer_engines/vllm && cd vllm && git checkout -f && cd .. && bash bash_install_code.sh && pip uninstall vllm -y && pip uninstall omni_infer -y && cd vllm && SETUPTOOLS_SCM_PRETEND_VERSION=0.9.0 VLLM_TARGET_DEVICE=empty pip install -e . --no-deps --no-build-isolation && cd ../../ && pip install -e . --no-deps --no-build-isolation && pip uninstall numpy -y && pip install numpy==1.26 --no-deps --no-build-isolation > ${LOG_PATH}/{{ inventory_hostname }}/pip.log && [ "${KV_CONNECTOR}" != "LMCacheConnectorV1" ] || { source ~/.bashrc && cd /workspace/omniinfer/omni/adaptors/lmcache/script && bash install.sh &> ${LOG_PATH}/install.log;}'
    docker_update_decode_code_cmd: >
      {{ docker_exec_cmd }}
      -e KV_CONNECTOR=$KV_CONNECTOR
      $DOCKER_NAME_D
      /bin/bash -c '. ~/.bashrc && cd /workspace/omniinfer/infer_engines && git config --global --add safe.directory /workspace/omniinfer/infer_engines/vllm && cd vllm && git checkout -f && cd .. && bash bash_install_code.sh && pip uninstall vllm -y && pip uninstall omni_infer -y && cd vllm && SETUPTOOLS_SCM_PRETEND_VERSION=0.9.0 VLLM_TARGET_DEVICE=empty pip install -e . --no-deps --no-build-isolation && cd ../../ && pip install -e . --no-deps --no-build-isolation && pip uninstall numpy -y && pip install numpy==1.26 --no-deps --no-build-isolation > ${LOG_PATH}/{{ inventory_hostname }}/pip.log && [ "${KV_CONNECTOR}" != "LMCacheConnectorV1" ] || { source ~/.bashrc && cd /workspace/omniinfer/omni/adaptors/lmcache/script && bash install.sh &> ${LOG_PATH}/install.log;}'
    docker_start_etcd_cmd: >
      {{ docker_exec_cmd }}
      -e KV_CONNECTOR=$KV_CONNECTOR
      $DOCKER_NAME_C
      /bin/bash -c '[ "${KV_CONNECTOR}" != "LMCacheConnectorV1" ] || { nohup etcd --listen-client-urls http://{{ ETCD_IP }}:{{ ETCD_PORT }} --advertise-client-urls http://{{ ETCD_IP }}:{{ ETCD_PORT }} &> ${LOG_PATH}/etcd.log &}'
    docker_start_mooncake_master_cmd: >
      {{ docker_exec_cmd }}
      -e KV_CONNECTOR=$KV_CONNECTOR
      $DOCKER_NAME_C
      /bin/bash -c '[ "${KV_CONNECTOR}" != "LMCacheConnectorV1" ] || { source ~/.bashrc && nohup mooncake_master -rpc_address={{ MOONCAKE_MASTER_IP }} -rpc_port={{ MOONCAKE_MASTER_PORT }} -metrics_port={{ MOONCAKE_METRICS_PORT }} -v 2 -eviction_high_watermark_ratio=0.8 -eviction_ratio=0.1 &> ${LOG_PATH}/mooncake_master.log &}'
  tasks:
    - name: generate container name.
      set_fact:
        ACTUAL_DOCKER_NAME_P: "{{ ansible_env.DOCKER_NAME_P }}_{{ inventory_hostname }}"
        ACTUAL_DOCKER_NAME_D: "{{ ansible_env.DOCKER_NAME_D }}_{{ inventory_hostname }}"
        ACTUAL_DOCKER_NAME_C: "{{ ansible_env.DOCKER_NAME_C }}_{{ inventory_hostname }}"
      when: "'P' in group_names or 'D' in group_names or 'C' in group_names"
      tags: always
      
    - name: Check and create new Prefill/Decode instances.
      block:
        - name: Check whether the container exists.
          shell: |
            docker inspect --format='{{"{{.Name}}"}}' \
            $DOCKER_NAME_P $DOCKER_NAME_D \
            2>/dev/null | grep -v '^$'
          register: existing_containers
          environment:
            DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
            DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
          failed_when: false
          changed_when: false

        - name: Show generated container name.
          debug:
            msg: "Generated container name: {{ existing_containers.stdout_lines }}"

        - name: Run container for new prefill instances.
          command: bash -c "{{ start_docker_cmd_p }}"
          environment:
            DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
          when: "existing_containers.stdout == '' and 'P' in group_names"

        - name: Run container for new decode instances.
          command: bash -c "{{ start_docker_cmd_d }}"
          environment:
            DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
          when: "existing_containers.stdout == '' and 'D' in group_names"

        - name: Create a directory to store the log.
          ansible.builtin.file:
            path: "{{ ansible_env.LOG_PATH }}/{{ inventory_hostname }}"
            state: directory
          when: "existing_containers.stdout == '' and ('P' in group_names or 'D' in group_names)"

        - name: Create a directory to store the code.
          ansible.builtin.file:
            path: "{{ ansible_env.CODE_PATH }}"
            state: directory
          when: existing_containers.stdout == ""

        - name: The executor synchronizes code to all instances.
          synchronize:
            src: "{{ ansible_env.CODE_PATH }}/omniinfer"
            dest: "{{ ansible_env.CODE_PATH }}/"
          when: "existing_containers.stdout == '' and ('P' in group_names or 'D' in group_names)"

        - name: Copy the code from the host machine into the container (prefill).
          command: bash -c "{{ docker_cp_prefill_code_cmd }}"
          environment: 
            DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
          when: "existing_containers.stdout == '' and 'P' in group_names"

        - name: Copy the code from the host machine into the container (decode).
          command: bash -c "{{ docker_cp_decode_code_cmd }}"
          environment: 
            DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
          when: "existing_containers.stdout == '' and 'D' in group_names"

        - name: docker_update_prefill_code_cmd.
          command: bash -c "{{ docker_update_prefill_code_cmd }}"
          environment:
            DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
          when: "existing_containers.stdout == '' and 'P' in group_names"

        - name: docker_update_decode_code_cmd.
          command: bash -c "{{ docker_update_decode_code_cmd }}"
          environment:
            DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
          when: "existing_containers.stdout == '' and 'D' in group_names"

        - name: Delete temporary script files.
          command: /bin/bash -c "rm -rf ${SCRIPTS_PATH}/*"
          register: cmd_result
          when: "existing_containers.stdout == '' and ('P' in group_names or 'D' in group_names)"

        - name: Register all values.
          set_fact:
            PREFILL_API_SERVER_LIST: >-
              {% set result=[] %}
              {% for host in groups['P']|default([]) %}
                {% set h=hostvars.get(host,{}) %}
                {% set ansible_host_val=h.ansible_host|default('') %}
                {% set host_ip_val=h.host_ip|default('') %}
                {% set api_port_val=h.api_port|default('9000') %}
                {% if ansible_host_val and host_ip_val and ansible_host_val == host_ip_val %}
                  {% set entry=ansible_host_val~':'~api_port_val %}
                  {% if entry not in result %}
                  {% set _=result.append(entry) %}
                  {% endif %}
                {% endif %}
              {% endfor %}
              {{ result|join(',') }}
            DECODE_API_SERVER_LIST: >-
              {% set result=[] %}
              {% for host in groups['D']|default([]) %}
                {% set h=hostvars.get(host,{}) %}
                {% set ip=h.ansible_host|default('') %}
                {% set port=h.api_port|default('9100') %}
                {% set num=h.ascend_rt_visible_devices.count(',')|default('0') %}
                {% if ip %}
                  {% set entry=ip~':'~port~'@'~num %}
                  {% if entry not in result %}
                    {% set _=result.append(entry) %}
                  {% endif %}
                {% endif %}
              {% endfor %}
              {{ result | join(',') }}
            PREFILL_POD_NUM: >-
              {{
                groups['P'] |
                map('extract', hostvars) |
                map(attribute='host_ip') |
                unique |
                length
              }}
            DECODE_POD_NUM: "{{ groups['D'] | length }}"
            DECODE_SERVER_IP_LIST: >-
              {% set host_list = [] %}
              {% for host in groups['D'] %}
                {% if hostvars[host].host_ip == hostvars[host].ansible_host %}
                  {% set _ = host_list.insert(0, hostvars[host].ansible_host) %}
                {% else %}
                  {% set _ = host_list.append(hostvars[host].ansible_host) %}
                {% endif %}
              {% endfor %}
              {{ host_list | join(',') }}
            DECODE_SERVER_ALL: "{{ groups['D'] | map('extract', hostvars) | map(attribute='ascend_rt_visible_devices') | select('defined') | join(',') }}"
            DECODE_SERVER_OFFSET: "{% set offsets = {} %}{% set ns = namespace(cnt=0) %}{% for host in groups['D']|default([]) %}{% set _ = offsets.update({host: ns.cnt}) %}{% set num=hostvars[host].ascend_rt_visible_devices.count(',')|default('0')|int %}{% set ns.cnt = ns.cnt + num + 1 %}{% endfor %}{{ offsets }}"
          run_once: yes
          delegate_to: localhost
          tags: always

        - name: Register values for prefill.
          set_fact:
            NODE_IP_LIST: >-
              {{
                groups['P'] |
                map('extract', hostvars) |
                selectattr('host_ip', '==', host_ip) |
                map(attribute='ansible_host') |
                unique |
                join(',')
              }}
            NNODES: >-
              {{
                groups['P'] |
                map('extract', hostvars) |
                selectattr('host_ip', '==', host_ip) |
                list |
                length
              }}
          when: "'P' in group_names"
          tags: always

        - name: Generate a script to kill all Python processes in the container.
          copy:
            content: "{{ kill_python_processes_cmd }}"
            dest: "$SCRIPTS_PATH/kill_python_processes.sh"
            mode: '0750'
          when: "existing_containers.stdout == '' and ('P' in group_names or 'D' in group_names)"

        - name: Generate a script to kill all Ray processes in the container. 
          copy:
            content: "{{ kill_ray_processes_cmd }}"
            dest: "$SCRIPTS_PATH/kill_ray_processes.sh"
            mode: '0750'
          when: "existing_containers.stdout == '' and ('P' in group_names or 'D' in group_names)"

        - name: Kill all Python processes in the container of prefill.
          command: /bin/bash -c "{{ docker_exec_cmd }} $DOCKER_NAME_P /bin/bash -c $SCRIPTS_PATH/kill_python_processes.sh"
          environment:
            DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
          failed_when: false
          no_log: true
          when: "existing_containers.stdout == '' and 'P' in group_names"

        - name: Kill all Ray processes in the container of prefill.
          command: /bin/bash -c "{{ docker_exec_cmd }} $DOCKER_NAME_P /bin/bash -c $SCRIPTS_PATH/kill_ray_processes.sh"
          environment:
            DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
          failed_when: false
          no_log: true
          when: "existing_containers.stdout == '' and 'P' in group_names"

        - name: Kill all Python processes in the container of decode.
          command: /bin/bash -c "{{ docker_exec_cmd }} $DOCKER_NAME_D /bin/bash -c $SCRIPTS_PATH/kill_python_processes.sh"
          environment:
            DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
          failed_when: false
          no_log: true
          when: "existing_containers.stdout == '' and 'D' in group_names"

        - name: Kill all Ray processes in the container of decode.
          command: /bin/bash -c "{{ docker_exec_cmd }} $DOCKER_NAME_D /bin/bash -c $SCRIPTS_PATH/kill_ray_processes.sh"
          environment:
            DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
          failed_when: false
          no_log: true
          when: "existing_containers.stdout == '' and 'D' in group_names"

        - name: Set mooncake & etcd configuration.
          set_fact:
            ETCD_IP: "{{ hostvars[groups['C'][0]]['ansible_host'] }}"
            ETCD_PORT: "{{ etcd_port }}"
            MOONCAKE_MASTER_IP: "{{ hostvars[groups['C'][0]]['ansible_host'] }}"
            MOONCAKE_MASTER_PORT: "{{ mooncake_master_port }}"
            MOONCAKE_METRICS_PORT: "{{ mooncake_metrics_port }}"
            PROXY_PORT: "{{ proxy_port }}"
          when: "existing_containers.stdout == '' and ('P' in group_names or 'D' in group_names) and etcd_port is defined and mooncake_master_port is defined and mooncake_metrics_port is defined"

        - name: Generate a mooncake config json.
          copy:
            content: "{{ generate_mooncake_config_json }}"
            dest: "$SCRIPTS_PATH/mooncake_config.json"
            mode: '0750'
          vars:
            HOST_IP: "{{ ansible_host }}"
          when: "existing_containers.stdout == '' and ('P' in group_names or 'D' in group_names) and etcd_port is defined and mooncake_master_port is defined and mooncake_metrics_port is defined"

        - name: Generate a lmcahce + mooncake config yaml.
          copy:
            content: "{{ generate_lmcache_mooncake_config_yml }}"
            dest: "$SCRIPTS_PATH/lmcache_mooncake_config.yml"
            mode: '0750'
          when: "existing_containers.stdout == '' and ('P' in group_names or 'D' in group_names) and etcd_port is defined and mooncake_master_port is defined and mooncake_metrics_port is defined"

        - name: Remove proc_trace.txt if it exists (P & D nodes)
          ansible.builtin.file:
            path: /tmp/process/proc_trace.txt
            state: absent
          when: "existing_containers.stdout == '' and ('P' in group_names or 'D' in group_names)"

        - name: Generate a script to run the vllm server for the prefill instances.
          copy:
            content: "{{ run_vllm_server_prefill_cmd }}"
            dest: "$SCRIPTS_PATH/vllm_run_for_p.sh"
            mode: '0750'
          when: "existing_containers.stdout == '' and 'P' in group_names"

        - name: Generate a script to run the vllm server for the decode instances.
          copy:
            content: "{{ run_vllm_server_decode_cmd }}"
            dest: "$SCRIPTS_PATH/vllm_run_for_d.sh"
            mode: '0750'
          vars:
            server_offset_dict: "{{ DECODE_SERVER_OFFSET }}"
          when: "existing_containers.stdout == '' and 'D' in group_names"

        - name: Get socket name for communication between prefill instances and decode instances.
          shell: |
            ip -4 route list 0/0 | awk '{print $5}' | head -1
          register: default_interface_result
          changed_when: false

        - name: Use a variable to store the socket name.
          set_fact:
            default_interface: "{{ default_interface_result.stdout }}"
          when: default_interface_result.stdout != ""

        - name: Run the Omniai service for decode instances.
          command: bash -c "{{ docker_start_vllm_cmd_d }}"
          environment:
            ROLE: "D"
            DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
            NODE_PORT: "{{ node_port }}"
            API_PORT: "{{ api_port }}"
            DECODE_SERVER_LIST: "{{ ascend_rt_visible_devices }}"
            SERVER_IP_LIST: "{{ DECODE_SERVER_IP_LIST | replace(' ', '') | trim }}"
            PREFILL_POD_NUM: "{{ PREFILL_POD_NUM }}"
            DECODE_POD_NUM: "{{ DECODE_POD_NUM }}"
            SOCKET_IFNAME: "{{ default_interface }}"
            NUM_SERVERS: "{{ ascend_rt_visible_devices.split(',') | length }}"
            HOST_IP: "{{ host_ip }}"
            DECODE_DATA_PARALLEL_SIZE: "{{ DECODE_SERVER_ALL }}"
            HOST: "{{ inventory_hostname }}"
            MOONCAKE_CONFIG_PATH: "$SCRIPTS_PATH/mooncake_config.json"
            LMCACHE_MOONCAKE_CONFIG_PATH: "$SCRIPTS_PATH/lmcache_mooncake_config.yml"
          when: "existing_containers.stdout == '' and 'D' in group_names"

        - name: Run the Omniai service for prefill instances.
          command: bash -c "{{ docker_start_vllm_cmd_p }}"
          environment:
            ROLE: "P"
            DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
            NODE_PORT: "{{ node_port }}"
            PREFILL_SERVER_LIST: "{{ ascend_rt_visible_devices }}"
            API_PORT: "{{ api_port }}"
            SERVER_IP_LIST: "{{ DECODE_SERVER_IP_LIST | replace(' ', '') | trim }}"
            PREFILL_POD_NUM: "{{ PREFILL_POD_NUM }}"
            SOCKET_IFNAME: "{{ default_interface }}"
            IP: "{{ ansible_host }}"
            HOST_IP: "{{ host_ip }}"
            KV_RANK: "{{ kv_rank }}"
            NODE_RANK: "{{ node_rank }}"
            PREFILL_TENSOR_PARALLEL_SIZE: "{{ ascend_rt_visible_devices.split(',') | length }}"
            NNODES: "{{ NNODES }}"
            NODE_IP_LIST: "{{ NODE_IP_LIST }}"
            MOONCAKE_CONFIG_PATH: "$SCRIPTS_PATH/mooncake_config.json"
            LMCACHE_MOONCAKE_CONFIG_PATH: "$SCRIPTS_PATH/lmcache_mooncake_config.yml"
          when: "existing_containers.stdout == '' and 'P' in group_names"

        - name: Ensure bind_cpu.sh is executable in the container
          ansible.builtin.shell:
            cmd: docker exec $DOCKER_NAME_P chmod +x /workspace/omniinfer/tools/scripts/bind_cpu.sh
          environment:
            DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
          when: "existing_containers.stdout == '' and 'P' in group_names"

        - name: Ensure bind_cpu.sh is executable in the container
          ansible.builtin.shell:
            cmd: docker exec $DOCKER_NAME_D chmod +x /workspace/omniinfer/tools/scripts/bind_cpu.sh
          environment:
            DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
          when: "existing_containers.stdout == '' and 'D' in group_names"
        
        - name: Prefill cpu_pin
          command: /bin/bash -c "{{ docker_exec_cmd }} $DOCKER_NAME_P /bin/bash -c /workspace/omniinfer/tools/scripts/bind_cpu.sh"
          environment:
            DOCKER_NAME_P: "{{ ACTUAL_DOCKER_NAME_P }}"
            SCRIPTS_PATH: "{{ ansible_env.SCRIPTS_PATH }}"
            ROLE: "P"
          become: yes
          when: 
            - "existing_containers.stdout == '' and 'P' in group_names"
            - "proc_bind_enabled | default(false)"
        
        - name: Decode cpu_pin
          command: /bin/bash -c "{{ docker_exec_cmd }} $DOCKER_NAME_D /bin/bash -c /workspace/omniinfer/tools/scripts/bind_cpu.sh"
          environment:
            DOCKER_NAME_D: "{{ ACTUAL_DOCKER_NAME_D }}"
            SCRIPTS_PATH: "{{ ansible_env.SCRIPTS_PATH }}"
            ROLE: "D"
          become: yes
          when: 
            - "existing_containers.stdout == '' and 'D' in group_names"
            - "proc_bind_enabled | default(false)"

      when: "'P' in group_names or 'D' in group_names"
      tags: add_node

    - name: Generate a script to kill all nginx processes in the container.
      copy:
        content: "{{ kill_nginx_processes_cmd }}"
        dest: "$SCRIPTS_PATH/kill_nginx_processes.sh"
        mode: '0750'
      when: "'C' in group_names"
      tags: run_proxy

    - name: Kill all Python processes in the container of proxy.
      command: /bin/bash -c "{{ docker_exec_cmd }} $DOCKER_NAME_C /bin/bash -c $SCRIPTS_PATH/kill_nginx_processes.sh"
      environment:
        DOCKER_NAME_C: "{{ ACTUAL_DOCKER_NAME_C }}"
      failed_when: false
      no_log: true
      when: "'C' in group_names"
      tags: run_proxy

    - name: Generate a script to run the global proxy server.
      copy:
        content: "{{ run_proxy_cmd }}"
        dest: "$SCRIPTS_PATH/run_proxy_server.sh"
        mode: '0750'
      when: "'C' in group_names"
      tags:
        - run_proxy

    - name: Run the global proxy server.
      command: bash -c "{{ docker_start_proxy_cmd_c }}"
      environment:
        DOCKER_NAME_C: "{{ ACTUAL_DOCKER_NAME_C }}"
      when: "'C' in group_names"
      tags:
        - run_proxy

    - name: Create a directory on the executor to store the log.
      ansible.builtin.file:
        path: "{{ ansible_env.LOG_PATH_IN_EXECUTOR }}/{{ inventory_hostname }}"
        state: directory
      when: "'P' in group_names or 'D' in group_names or 'C' in group_names"
      delegate_to: localhost
      tags:
        - fetch_log

    - name: Forward logs from all machines to the executor.
      ansible.builtin.synchronize:
        mode: pull
        src: "{{ ansible_env.LOG_PATH }}/{{ inventory_hostname }}/"
        dest: "{{ ansible_env.LOG_PATH_IN_EXECUTOR }}/{{ inventory_hostname }}/"
      when: "'P' in group_names or 'D' in group_names or 'C' in group_names"
      tags:
        - fetch_log
